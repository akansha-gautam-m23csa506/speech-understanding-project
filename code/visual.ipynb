{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Understanding\n",
    "## Robust Automatic Speech Recognition in Noisy Environment with Lip-Reading Assistance\n",
    "### Akansha Gautam    (M23CSA506)\n",
    "### Anchit Mulye      (M23CSA507)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "elif torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load AV Speech Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVSpeech Train Dataset Shape: (2621845, 5)\n",
      "AVSpeech Test Dataset Shape: (183273, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>youtube_id</th>\n",
       "      <th>start_segment</th>\n",
       "      <th>end_segment</th>\n",
       "      <th>x_coordinate</th>\n",
       "      <th>y_coordinate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CJoOwXcjhds</td>\n",
       "      <td>233.266</td>\n",
       "      <td>239.367000</td>\n",
       "      <td>0.780469</td>\n",
       "      <td>0.670833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AvWWVOgaMlk</td>\n",
       "      <td>90.000</td>\n",
       "      <td>93.566667</td>\n",
       "      <td>0.586719</td>\n",
       "      <td>0.311111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    youtube_id  start_segment  end_segment  x_coordinate  y_coordinate\n",
       "0  CJoOwXcjhds        233.266   239.367000      0.780469      0.670833\n",
       "1  AvWWVOgaMlk         90.000    93.566667      0.586719      0.311111"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avspeech_train_path = '/Users/akanshagautam/Documents/MTech/Speech Understanding/Project/dataset/avspeech/avspeech_train.csv'\n",
    "avspeech_test_path = '/Users/akanshagautam/Documents/MTech/Speech Understanding/Project/dataset/avspeech/avspeech_test.csv'\n",
    "names = [\"youtube_id\", \"start_segment\", \"end_segment\", \"x_coordinate\", \"y_coordinate\"]\n",
    "\n",
    "avspeech_train_df = pd.read_csv(avspeech_train_path, names=names)\n",
    "avspeech_test_df = pd.read_csv(avspeech_test_path, names=names)\n",
    "\n",
    "print(f\"AVSpeech Train Dataset Shape: {avspeech_train_df.shape}\")\n",
    "print(f\"AVSpeech Test Dataset Shape: {avspeech_test_df.shape}\")\n",
    "\n",
    "avspeech_train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total wav files: 99\n",
      "Final Dataset Shape: (1481, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>youtube_id</th>\n",
       "      <th>start_segment</th>\n",
       "      <th>end_segment</th>\n",
       "      <th>x_coordinate</th>\n",
       "      <th>y_coordinate</th>\n",
       "      <th>video_path</th>\n",
       "      <th>text_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-A9gdf3j2xo</td>\n",
       "      <td>295.165000</td>\n",
       "      <td>298.165000</td>\n",
       "      <td>0.507812</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>/Users/akanshagautam/Documents/MTech/Speech Un...</td>\n",
       "      <td>/Users/akanshagautam/Documents/MTech/Speech Un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>QoQF8N5ZsQA</td>\n",
       "      <td>240.006433</td>\n",
       "      <td>244.961389</td>\n",
       "      <td>0.450781</td>\n",
       "      <td>0.358333</td>\n",
       "      <td>/Users/akanshagautam/Documents/MTech/Speech Un...</td>\n",
       "      <td>/Users/akanshagautam/Documents/MTech/Speech Un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sujFCXbYkMo</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>34.466667</td>\n",
       "      <td>0.528906</td>\n",
       "      <td>0.477778</td>\n",
       "      <td>/Users/akanshagautam/Documents/MTech/Speech Un...</td>\n",
       "      <td>/Users/akanshagautam/Documents/MTech/Speech Un...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    youtube_id  start_segment  end_segment  x_coordinate  y_coordinate  \\\n",
       "0  -A9gdf3j2xo     295.165000   298.165000      0.507812      0.233333   \n",
       "1  QoQF8N5ZsQA     240.006433   244.961389      0.450781      0.358333   \n",
       "2  sujFCXbYkMo      30.000000    34.466667      0.528906      0.477778   \n",
       "\n",
       "                                          video_path  \\\n",
       "0  /Users/akanshagautam/Documents/MTech/Speech Un...   \n",
       "1  /Users/akanshagautam/Documents/MTech/Speech Un...   \n",
       "2  /Users/akanshagautam/Documents/MTech/Speech Un...   \n",
       "\n",
       "                                           text_path  \n",
       "0  /Users/akanshagautam/Documents/MTech/Speech Un...  \n",
       "1  /Users/akanshagautam/Documents/MTech/Speech Un...  \n",
       "2  /Users/akanshagautam/Documents/MTech/Speech Un...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_DIR = '/Users/akanshagautam/Documents/MTech/Speech Understanding/Project/dataset/avspeech/train'\n",
    "temp = []\n",
    "\n",
    "for folder in os.listdir(BASE_DIR):\n",
    "    folder_path = os.path.join(BASE_DIR, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        video_path = \"\"\n",
    "        text_path = \"\"\n",
    "        for file in os.listdir(folder_path):\n",
    "            if file.endswith(\".mp4\"):\n",
    "                video_path = os.path.join(folder_path, file)\n",
    "            if file.endswith(\".srt\"):\n",
    "                text_path = os.path.join(folder_path, file)\n",
    "        temp.append({\n",
    "            \"youtube_id\": folder,\n",
    "            \"video_path\": video_path,\n",
    "            \"text_path\": text_path\n",
    "        })\n",
    "\n",
    "wav_df = pd.DataFrame(temp)\n",
    "print(f\"Total wav files:\", wav_df.shape[0])\n",
    "\n",
    "given_avspeech_df = pd.merge(avspeech_train_df, wav_df, on=\"youtube_id\", how=\"inner\")\n",
    "print(f\"Final Dataset Shape: {given_avspeech_df.shape}\")\n",
    "\n",
    "given_avspeech_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crop the videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x2d6a18dd0] mmco: unref short failure\n"
     ]
    }
   ],
   "source": [
    "CROP_SIZE = 400\n",
    "FPS = 25\n",
    "\n",
    "output_dir = \"/Users/akanshagautam/Documents/MTech/Speech Understanding/Project/dataset/avspeech/mouth_crops\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for idx, row in given_avspeech_df.iterrows():\n",
    "    video_path = row['video_path']\n",
    "    youtube_id = row['youtube_id']\n",
    "    start_time = row['start_segment']\n",
    "    end_time = row['end_segment']\n",
    "    x_center = row['x_coordinate']\n",
    "    y_center = row['y_coordinate']\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        continue\n",
    "\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS) or FPS\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    start_frame = int(start_time * fps)\n",
    "    end_frame = int(end_time * fps)\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "    current_frame = start_frame\n",
    "\n",
    "    segment_id = f\"{youtube_id}_{int(start_time)}_{int(end_time)}\"\n",
    "    video_output_dir = os.path.join(output_dir, segment_id)\n",
    "    os.makedirs(video_output_dir, exist_ok=True)\n",
    "\n",
    "    while current_frame <= end_frame:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        x_px = int(x_center * width)\n",
    "        y_px = int(y_center * height)\n",
    "\n",
    "        half_crop = CROP_SIZE // 2\n",
    "        x1 = max(x_px - half_crop, 0)\n",
    "        y1 = max(y_px - half_crop, 0)\n",
    "        x2 = min(x_px + half_crop, width)\n",
    "        y2 = min(y_px + half_crop, height)\n",
    "        mouth_crop = frame[y1:y2, x1:x2]\n",
    "\n",
    "        crop_filename = os.path.join(video_output_dir, f\"{current_frame}.png\")\n",
    "        cv2.imwrite(crop_filename, mouth_crop)\n",
    "\n",
    "        current_frame += 1\n",
    "\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
